{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "**1**: Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a [Lorem Ipsum](https://www.lipsum.com/) of 3 paragraphs in a txt file using python, each paragraph delimited by two new line.\n",
    "\n",
    "**2**: Update the txt file by removing the first paragraph.\n",
    "\n",
    "**3**: Create a dict from the paper of [lecun et al.](https://www.researchgate.net/publication/277411157_Deep_Learning) and [goodfellow et al.](https://arxiv.org/abs/1406.2661) with authors, title, affiliations.\n",
    "\n",
    "**4**: Save the previously created dict in the JSON format and load it back.\n",
    "\n",
    "**5**: Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?\n",
    "\n",
    "**6**: Parse the xml_file2 in the same way as in the lecture. put infos in a dict and save it in a json file.\n",
    "\n",
    "**7**: Download an image of your choice and save it in either jpg or png.\n",
    "\n",
    "**8**: From the data/Chap2/data_world.json file, create a set of publisher type.\n",
    "\n",
    "**9**: From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json.\n",
    "\n",
    "**10**: From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1 \n",
    "from lorem import paragraph\n",
    "\n",
    "with open('data/Chap2/lorem.txt', 'w+', encoding= \"utf-8\") as lo:\n",
    "    text = [paragraph() + '\\n' + '\\n' for _ in range(3)]\n",
    "    lo.writelines(text)\n",
    "    lo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 \n",
    "with open('data/Chap2/lorem.txt', 'r+', encoding= \"utf-8\") as lo:\n",
    "    par = lo.readlines()\n",
    "    \n",
    "    lo.seek(0)\n",
    "    lo.writelines(par[2:])\n",
    "    lo.truncate() \n",
    "    lo.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3 \n",
    "papers = {1 : {'authors':['Yann LeCun', 'Yoshua Bengio', 'Geoffrey Hinton'], 'title':'Deep learning', 'affiliations':['Université de Montréal']},\n",
    "        2 : {'authors': ['Ian J.Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio'], 'title': 'Generative Adversarial Nets', 'affiliations': ['Université de Montréal']}}\n",
    "\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4 \n",
    "import json\n",
    "\n",
    "with open('data/Chap2/papers.json', 'w+') as file:\n",
    "    json.dump(papers, file)\n",
    "    file.close()\n",
    "\n",
    "with open('data/Chap2/papers.json', 'r') as file:\n",
    "    json_papers = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "print(json_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "import pickle\n",
    "\n",
    "with open('data/Chap2/papers.pkl', 'wb+') as file:\n",
    "    pickle.dump(papers, file)\n",
    "    file.close()\n",
    "\n",
    "with open('data/Chap2/papers.pkl', 'rb') as file:\n",
    "    pickle_papers = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "print(pickle_papers)\n",
    "\n",
    "'''\n",
    "Le format n'est pas lisible car pickle encode en binaire\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6 \n",
    "import lxml.etree\n",
    "import json\n",
    "\n",
    "root = lxml.etree.parse(\"data/Chap2/xml_file2.nxml\")\n",
    "\n",
    "dict = {'date':root.xpath('//date//text()'),\n",
    "        'hour':root.xpath('//hour//text()'),\n",
    "        'to':root.xpath('//to//text()'),\n",
    "        'from':root.xpath('//from//text()'),\n",
    "        'body':root.xpath('//body//text()')}\n",
    "\n",
    "with open('data/Chap2/json_file2.json', 'w+', encoding='utf-8') as file:\n",
    "    json.dump(papers, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7 \n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "im = Image.open(requests.get(\"https://i.pinimg.com/originals/46/66/77/46667760b843a1cacc10fddc5bdac006.png\", stream=True).raw)\n",
    "im.save(\"data/Chap2/image_nasique.png\", \"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "with open('data/Chap2/data_world.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "_set = {entry[\"publisher\"].get('@type') for entry in tqdm.tqdm(data) if entry[\"publisher\"].get('@type')}\n",
    "print(_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9 \n",
    "[entry.pop('accessLevel', None) for entry in tqdm.tqdm(data)]\n",
    "\n",
    "with open('data/Chap2/data_world_cleaned.json', 'w+') as file:\n",
    "    json.dump(data, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 10\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('data/Chap2/data_world.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for entry in data:\n",
    "    access_level = entry.get('accessLevel')\n",
    "    accrual_periodicity = entry.get('accrualPeriodicity')\n",
    "    matrix[access_level][accrual_periodicity] += 1\n",
    "#{entry.get('accrualPeriodicity') for entry in data}\n",
    "#{entry.get(\"accessLevel\") for entry in data}\n",
    "\n",
    "matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
